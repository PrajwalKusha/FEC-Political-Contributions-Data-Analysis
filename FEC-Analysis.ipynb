{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0158833e-d0d6-4f55-8956-a1360f9e9c78",
   "metadata": {},
   "source": [
    "### Detailed Analysis of Individual Contributions to Presidential, Senate, and House Committees\n",
    "\n",
    "\n",
    "**Introduction:** In this project, we conduct an analysis of individual contributions to Federal Committees such as presidential, senate, and house committees for the period from June 20, 2024, to July 24, 2024. This analysis will involve data collection, data processing, and data analysis using Apache Spark. The goal is to gain insights into the patterns and trends of individual contributions to political committees during this period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1e390-4824-4d41-9944-aece089ffe60",
   "metadata": {},
   "source": [
    "**Project Submission by Prajwal Kusha**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2db37-651f-4be0-b6ae-8dddb6dce6f9",
   "metadata": {},
   "source": [
    "### Data Collection (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae1842-ad0c-4f1a-b3df-132dc4cc22e8",
   "metadata": {},
   "source": [
    "#### Step 1: Uploading the Data and unzipping it: June and July Data\n",
    "\n",
    "Given that the FEC data downloaded from the FEC website is huge, approximately **3.37 GB**. We zip the required files and upload it on the instance to proceed with the next steps. \n",
    "* The data to be uploaded are named according to the month in the ZIP file downloaded from the FEC website.\n",
    "* We proceed uploading the following data as available: **itcont_2024_20240620_20240709.txt, itcont_2024_20240710_20240723.txt** and the header file named **indiv_header_file.csv**\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "* `!unzip` A shell command to unzip (extract) a compressed .zip file.\n",
    "* `-o`: Overwrites existing files in the destination directory without prompting for confirmation.\n",
    "* `itcont_2024_20240620_20240709.txt.zip`: The input file to be unzipped, which contains data for June contributions.\n",
    "* `-d ./Fec_Jun`: Specifies the destination directory (./Fec_Jun) where the unzipped files will be extracted. If the directory does not exist, it will be created automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b2c7ee-50c3-45af-b9ee-eb9d438e4891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  itcont_2024_20240620_20240709.txt.zip\n",
      "  inflating: ./Fec_Jun/itcont_2024_20240620_20240709.txt  \n",
      "  inflating: ./Fec_Jun/__MACOSX/._itcont_2024_20240620_20240709.txt  \n",
      "Archive:  itcont_2024_20240710_20240723.txt.zip\n",
      "  inflating: ./Fec_Jul/itcont_2024_20240710_20240723.txt  \n",
      "  inflating: ./Fec_Jul/__MACOSX/._itcont_2024_20240710_20240723.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o itcont_2024_20240620_20240709.txt.zip -d ./Fec_Jun\n",
    "!unzip -o itcont_2024_20240710_20240723.txt.zip -d ./Fec_Jul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0594c-e9e2-4980-8f26-b2d5645ba778",
   "metadata": {},
   "source": [
    "#### Step 2: Removing the zip files: \n",
    "We do this so that we can save space on the instance. \n",
    "\n",
    "**Code Explanation:**\n",
    "* `!rm` - removes the files mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b151b96a-9062-48db-9824-61a3ed4d121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm itcont_2024_20240620_20240709.txt.zip\n",
    "!rm itcont_2024_20240710_20240723.txt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05de60-ff40-46f6-b5d1-668d345819eb",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6874010-a502-40aa-9c13-ee5ae3378695",
   "metadata": {},
   "source": [
    "#### Step 3: Converting the unzipped .txt file to .csv - June, July Data\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "* `!csvformat` Is a utility from the csvkit package used for manipulating and reformatting CSV files\n",
    "* `-d`Specifies the delimiter used in the input file.\n",
    "* `\"|\"`Indicates that the input file is pipe-delimited (| separates each field in the file)\n",
    "* `>` Sends the output of the csvformat command to a new file rather than displaying it in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e09a21d-fed7-497f-8320-c4832d20dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!csvformat -d \"|\" Fec_Jun/itcont_2024_20240620_20240709.txt > Fec_Jun/June24.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe80321d-646e-4490-837d-8e9cb02feb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!csvformat -d \"|\" Fec_Jul/itcont_2024_20240710_20240723.txt > Fec_Jul/July24.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c0a3a-7e10-45b6-b040-0a06e5dfee06",
   "metadata": {},
   "source": [
    "#### Step 4: Word count of the actual (.txt) file and the converted (.csv) file\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "* `wc` Word Count is usually used to count lines, characters and words in a file. \n",
    "* `-l` Specifies that the command should count only the number of lines in the file.\n",
    "\n",
    "**Inference: We see that the conversion has been successful. Both the files have the same number of lines**\n",
    "* June24 = 1944602\n",
    "* July24 = 1902192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6581dbd6-4d60-46b9-a9be-1202c3341475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1944602 Fec_Jun/itcont_2024_20240620_20240709.txt\n",
      "1944602 Fec_Jun/June24.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l Fec_Jun/itcont_2024_20240620_20240709.txt\n",
    "!wc -l Fec_Jun/June24.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f117b1d3-b77a-4b7c-8119-ac28d9a0efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1902192 Fec_Jul/itcont_2024_20240710_20240723.txt\n",
      "1902192 Fec_Jul/July24.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l Fec_Jul/itcont_2024_20240710_20240723.txt\n",
    "!wc -l Fec_Jul/July24.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8a963-3435-4d8f-98fd-f661c07bce7b",
   "metadata": {},
   "source": [
    "#### Step 5: Combine the header and data files using concatenate\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "* `!cat` The cat command is used combine multiple files into a single file.\n",
    "* The cat takes the header file and stacks on top of the June24 and July24 files and combines all 3 to provide the JunJul_24 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a909033-57f2-48f5-99f1-a77a92607825",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat indiv_header_file.csv Fec_Jun/June24.csv Fec_Jul/July24.csv > JunJul_24.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a113c-4d99-4e7e-90ba-9d3a45d9959b",
   "metadata": {},
   "source": [
    "#### Step 6: Run word count to check if the combining was successful. \n",
    "\n",
    "* `!wc -l` Is used to count number of lines. \n",
    "* We can observe that 1944602 + 1902192 + 1 = **3846795 = JunJul_24** (Hence combining was successful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "429360a0-b1cb-4273-abb1-ac4f15f9d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3846795 JunJul_24.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l JunJul_24.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def158e8-ce0b-4591-9c47-9bc4546eb42d",
   "metadata": {},
   "source": [
    "#### Step 7: Understanding the Data - Print header column of the data\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "* `!csvcut` The command displays the column names in a file\n",
    "* `-n` Lists all column headers (names) in the CSV file along with their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd25f658-513c-43cc-a5b0-99fd5a5fca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: CMTE_ID\n",
      "  2: AMNDT_IND\n",
      "  3: RPT_TP\n",
      "  4: TRANSACTION_PGI\n",
      "  5: IMAGE_NUM\n",
      "  6: TRANSACTION_TP\n",
      "  7: ENTITY_TP\n",
      "  8: NAME\n",
      "  9: CITY\n",
      " 10: STATE\n",
      " 11: ZIP_CODE\n",
      " 12: EMPLOYER\n",
      " 13: OCCUPATION\n",
      " 14: TRANSACTION_DT\n",
      " 15: TRANSACTION_AMT\n",
      " 16: OTHER_ID\n",
      " 17: TRAN_ID\n",
      " 18: FILE_NUM\n",
      " 19: MEMO_CD\n",
      " 20: MEMO_TEXT\n",
      " 21: SUB_ID\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n JunJul_24.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85fdf9-4f54-4abc-8e2b-3566e5ce8165",
   "metadata": {},
   "source": [
    "#### Step 8: Understanding the data - Print top 10 rows of the data. \n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "* `!head`is a shell command used to display the beginning (first few lines) of a file.\n",
    "* `-n 10`Specifies the number of lines to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9896d54c-f716-4cc2-93cd-ce51d8e439cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMTE_ID,AMNDT_IND,RPT_TP,TRANSACTION_PGI,IMAGE_NUM,TRANSACTION_TP,ENTITY_TP,NAME,CITY,STATE,ZIP_CODE,EMPLOYER,OCCUPATION,TRANSACTION_DT,TRANSACTION_AMT,OTHER_ID,TRAN_ID,FILE_NUM,MEMO_CD,MEMO_TEXT,SUB_ID\n",
      "C00849273,T,TER,P2024,202407159660924932,22Y,IND,\"REPASS, JAMES\",HILLSBORO,VA,20132,,,06202024,400,,SB28A.5248,1802122,,,4071620241974571456\n",
      "C00828541,A,M7,P2024,202408159666207641,15,IND,\"MANSFIELD, RISA\",COLUMBUS,MS,39705,RETIRED,RETIRED,06202024,125,,SA17A.121694463,1810271,,,4081620242013476924\n",
      "C00828541,A,M7,P2024,202408159666207641,15,IND,\"MANTZOURANIS, VASILIKE\",KENSINGTON,MD,20895,RETIRED,RETIRED,06202024,150,,SA17A.121693527,1810271,,,4081620242013476925\n",
      "C00828541,A,M7,P2024,202408159666209186,15,IND,\"SCOTT, HORACE EDWARD\",HANCEVILLE,AL,35077,HORACE F SEPTT,RETAIR,06202024,100,,SA17A.121694767,1810271,,,4081620242013481558\n",
      "C00828541,A,M7,P2024,202408159666204850,15,IND,\"BRUSA, S E\",LODI,CA,95242,HOMEMAKER,HOMEMAKER,06202024,100,,SA17A.121694255,1810271,,,4081620242013468550\n",
      "C00828541,A,M7,P2024,202408159666204852,15,IND,\"BRUTON, JAMES A\",CLIFTON,VA,20124,WILLIAMS AND CONNELLY LLPO,LAWYER,06202024,1500,,SA17A.121693818,1810271,,,4081620242013468558\n",
      "C00828541,A,M7,P2024,202408159666206554,15,IND,\"HERDENER, CURTIS\",MERRITT ISLAND,FL,32953,RETIRED,RETIRED,06202024,500,,SA17A.121693776,1810271,,,4081620242013473664\n",
      "C00828541,A,M7,P2024,202408159666210107,15,IND,\"WEINBERGER, LUCILLE\",LAS VEGAS,NV,89144,,,06202024,100,,SA17A.121696063,1810271,,,4081620242013484321\n",
      "C00828541,A,M7,P2024,202408159666209334,15,IND,\"SIMPSON, TOMMY MAC\",COLLINSVILLE,AL,35961,,,06202024,500,,SA17A.121693670,1810271,,,4081620242013482002\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 JunJul_24.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4b98a-62a7-4a00-bb6f-2744a4c6bd05",
   "metadata": {},
   "source": [
    "#### Step 9: Initializing Spark Environment \n",
    "\n",
    "**Code Explanation**\n",
    "* `import findspark` A Python library used to locate and initialize Apache Spark in your environment.\n",
    "* `findspark.init()` Initializes Spark in the current Python environment.\n",
    "* `from pyspark import SparkContext` The primary entry point for using PySpark's RDD (Resilient Distributed Dataset) API.\n",
    "* `spark = SparkContext(appName='20241029')` Assigns a name to your Spark application, which will appear in the Spark UI or logs for tracking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abc6cf94-b78b-4dea-8203-2078421c20c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/25 23:01:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-21-207.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>20241029</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=20241029>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "spark = SparkContext(appName='20241029')\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc815f-5830-4d41-98c0-17500599457f",
   "metadata": {},
   "source": [
    "#### Step 10: Loading file onto Spark RDD \n",
    "\n",
    "* `FEC` The variable FEC stores the resulting RDD after loading the file\n",
    "* `spark.textFile` A method provided by the SparkContext (spark) to read text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194b8e3d-5da1-4da5-ad82-4fdd1ea0d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEC = spark.textFile('JunJul_24.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242da4a-ec39-4a18-b308-b5fbf7b3b404",
   "metadata": {},
   "source": [
    "#### Step 11: Run word count to see if data load was successful\n",
    "\n",
    "* `%time` Provides the time in which the task was completed\n",
    "* `FEC.count()` Counts the number of lines from the data loaded on spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91f6eb1-1564-49ef-98ec-2bcfd468e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>  (21 + 1) / 22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 ms, sys: 5.1 ms, total: 23 ms\n",
      "Wall time: 5.82 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "3846795"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time FEC.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab1a05-ddb5-4729-9c1e-5500017589bc",
   "metadata": {},
   "source": [
    "### Data Analysis Part 1: RDD Processing (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5653c655-b6ce-47e0-91df-d9604d419875",
   "metadata": {},
   "source": [
    "#### 1. The top 10 states in terms of the total count of contributions.\n",
    "\n",
    "**Code Explanation**\n",
    "\n",
    "* `from operator import add` The add function is used to sum values.\n",
    "* `header = FEC.first()` Retrieves the first row of the RDD, which is assumed to be the header of the CSV file.\n",
    "* `FEC.filter(lambda row: row != header)` Filters out the header row from the RDD, ensuring only data rows are processed.\n",
    "* `row.replace('\"', '')` Removes any double quotes (\") from the row to avoid issues with splitting or processing.\n",
    "* `row.split(',')` Splits each line of the CSV into a list of columns based on the comma (,) delimiter.\n",
    "* `cols[10]` Refers to the column corresponding to STATE. Ensures the STATE column is not empty ('') or None.\n",
    "* `.reduceByKey(add)` Aggregates the values for each state (key) by summing them using the add function.\n",
    "* `-pair[1])` Orders the states by the total count of contributions (pair[1]) in descending order (-pair[1]).\n",
    "\n",
    "**Note:** We know that Pyspark follows zero-based indexing for column positions in DataFrames and RDDs. This means that the first column is indexed as 0, the second column as 1, and so on. So ideally our state column should be column number 9 from our data. But in our code we have used column 10 to get the output, this is because the column 'NAME' has (\"\") which is wrongly predicted by pyspark and is hence considering the (\"\") to be a seperate column all together. Hence the columns after 'NAME' have an increment of 1 to them. Making column 'STATE' col[10] instead of 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "460eec18-0411-4eb6-af28-c4eafd1b86dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============>                                          (6 + 2) / 22]24/11/25 23:01:25 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 3:======================================================>  (21 + 1) / 22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 States by Total Count of Contributions:\n",
      "CA\t564941\n",
      "TX\t324187\n",
      "FL\t279517\n",
      "NY\t225839\n",
      "WA\t140701\n",
      "PA\t130224\n",
      "VA\t125452\n",
      "IL\t124756\n",
      "AZ\t114972\n",
      "OH\t109487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "header = FEC.first()\n",
    "top10 = FEC.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.replace('\"', '')) \\\n",
    "    .map(lambda row: row.split(',')) \\\n",
    "    .filter(lambda cols: cols[10] is not None and cols[10] != '') \\\n",
    "    .map(lambda cols: (cols[10], 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .takeOrdered(10, key=lambda pair: -pair[1])\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 States by Total Count of Contributions:\")\n",
    "for state, count in top10:\n",
    "    print(f\"{state}\\t{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb6639-74a9-4518-9f53-4c1ae142c58a",
   "metadata": {},
   "source": [
    "#### 2. The top 10 states in terms of the total (i.e., sum) amount of contributions\n",
    "\n",
    "**Code Explanation**\n",
    "\n",
    "* `cols: cols[10] is not None and cols[10] != ''` cols[10] (state column) is not empty or None.\n",
    "* `cols[15] is not None and cols[15] != ''` cols[15] (transaction amount) is not empty or None.\n",
    "* `replace('.', '', 1)` Removes the decimal point for validation.\n",
    "* `isdigit()` Checks if the resulting string is numeric.\n",
    "* `float(cols[15]` The contribution amount from cols[15], converted to a float.\n",
    "* `:,.2f` The total contribution amount formatted with commas and two decimal places\n",
    "\n",
    "**NOTE:** As mentioned earlier, every column after 'NAME' is incremented with 1. Hence 'TRANSACTION_AMT' is col[15]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "757b266a-8850-4679-8280-4949872ab9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==============================================>          (18 + 2) / 22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 states by sum of contributions:\n",
      "CA\t64,848,952,933.00\n",
      "TX\t55,447,769,793.00\n",
      "IL\t34,138,523,773.00\n",
      "NY\t30,559,012,259.00\n",
      "FL\t26,209,564,812.00\n",
      "GA\t23,380,003,290.00\n",
      "VA\t21,875,285,070.00\n",
      "PA\t21,375,962,094.00\n",
      "NJ\t20,531,652,271.00\n",
      "MA\t18,482,278,923.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "header = FEC.first()\n",
    "\n",
    "top10_sum = (\n",
    "    FEC.filter(lambda row: row != header)  \n",
    "    .map(lambda row: row.replace('\"', '')) \n",
    "    .map(lambda row: row.split(','))  \n",
    "    .filter(lambda cols: cols[10] is not None and cols[10] != '' and cols[15] is not None and cols[15] != '')  \n",
    "    .filter(lambda cols: cols[15].replace('.', '', 1).isdigit())  \n",
    "    .map(lambda cols: (cols[10], float(cols[15])))  \n",
    "    .reduceByKey(add)  \n",
    "    .takeOrdered(10, key=lambda pair: -pair[1])\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nTop 10 states by sum of contributions:\")\n",
    "for state, total in top10_sum:\n",
    "    print(f\"{state}\\t{total:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97359f28-43dc-42b4-ab68-227fbfcd7f22",
   "metadata": {},
   "source": [
    "### Part 2: Data Frame API (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f19c5-5c8b-4620-8e42-44fd5228eb6a",
   "metadata": {},
   "source": [
    "#### Step 12: Proceed with initializing SQL on spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0f65917-83c3-4093-9628-e8307f0ff903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/spark/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x763b757823f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SQLContext\n",
    "sqlc = SQLContext(spark)\n",
    "sqlc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8fd6f-c39a-44d7-b7fe-49771911bbcb",
   "metadata": {},
   "source": [
    "#### Step 13: Loading file on to the Dataframe\n",
    "\n",
    "**Code Explanation**\n",
    "* `sqlc.read.csv` Reads the file JunJul_24.csv into a Spark DataFrame.\n",
    "* `header=True` Indicates that the first row of the CSV file contains column names (headers).\n",
    "* `inferSchema=False` With inferSchema=False, all columns are read as strings, regardless of their actual data type.\n",
    "\n",
    "**Why use inferSchema=False**\n",
    "* **Performance Optimization:** Having inferSchema false is faster especially for large datasets, because it avoids the overhead of reading a portion of the data for type inference.\n",
    "* **Consistency in Data Types:** A column might contain mostly numeric data but include some non-numeric values (e.g., \"N/A\" or \"NULL\"). If inferSchema=True, Spark may fail to parse such values or throw errors during operations.\n",
    "* Given that our data contains null values, we choose to call inferSchema=False, as we can explicitly define the schema later using the schema parameter or by casting columns manually.\n",
    "\n",
    "**NOTE:** From the data available, we see that **Transaction_DT** is in the format of **'06202024'**, but when **inferSchema=True** the data is interpreted as **'6202024'**. This is the major reason for us to call inferSchema=False, as we can **define the datatype manually** to provide the **most accurate output possible.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02c7dd64-f1d4-4a25-9164-9a5f20e685e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEC = sqlc.read.csv(\"JunJul_24.csv\", header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca67d8-4f06-4ad8-bb91-b39cda0ecae0",
   "metadata": {},
   "source": [
    "#### Step 14: Count the number of lines after loading data onto the Dataframe\n",
    "\n",
    "* We can see that this matches with the number of lines in the actual file (JunJul_24.csv), hence data loading was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d812e4c6-d880-40d4-ad80-31a0e089022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "3846794"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEC.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed416e4d-c88e-44ec-8493-80c631598302",
   "metadata": {},
   "source": [
    "#### Step 15: Print first 5 rows of data. \n",
    "\n",
    "* Run this to identify any errors if occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0c8386-1ef4-4b0d-a76d-3325cea47b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CMTE_ID='C00849273', AMNDT_IND='T', RPT_TP='TER', TRANSACTION_PGI='P2024', IMAGE_NUM='202407159660924932', TRANSACTION_TP='22Y', ENTITY_TP='IND', NAME='REPASS, JAMES', CITY='HILLSBORO', STATE='VA', ZIP_CODE='20132', EMPLOYER=None, OCCUPATION=None, TRANSACTION_DT='06202024', TRANSACTION_AMT='400', OTHER_ID=None, TRAN_ID='SB28A.5248', FILE_NUM='1802122', MEMO_CD=None, MEMO_TEXT=None, SUB_ID='4071620241974571456'),\n",
       " Row(CMTE_ID='C00828541', AMNDT_IND='A', RPT_TP='M7', TRANSACTION_PGI='P2024', IMAGE_NUM='202408159666207641', TRANSACTION_TP='15', ENTITY_TP='IND', NAME='MANSFIELD, RISA', CITY='COLUMBUS', STATE='MS', ZIP_CODE='39705', EMPLOYER='RETIRED', OCCUPATION='RETIRED', TRANSACTION_DT='06202024', TRANSACTION_AMT='125', OTHER_ID=None, TRAN_ID='SA17A.121694463', FILE_NUM='1810271', MEMO_CD=None, MEMO_TEXT=None, SUB_ID='4081620242013476924'),\n",
       " Row(CMTE_ID='C00828541', AMNDT_IND='A', RPT_TP='M7', TRANSACTION_PGI='P2024', IMAGE_NUM='202408159666207641', TRANSACTION_TP='15', ENTITY_TP='IND', NAME='MANTZOURANIS, VASILIKE', CITY='KENSINGTON', STATE='MD', ZIP_CODE='20895', EMPLOYER='RETIRED', OCCUPATION='RETIRED', TRANSACTION_DT='06202024', TRANSACTION_AMT='150', OTHER_ID=None, TRAN_ID='SA17A.121693527', FILE_NUM='1810271', MEMO_CD=None, MEMO_TEXT=None, SUB_ID='4081620242013476925'),\n",
       " Row(CMTE_ID='C00828541', AMNDT_IND='A', RPT_TP='M7', TRANSACTION_PGI='P2024', IMAGE_NUM='202408159666209186', TRANSACTION_TP='15', ENTITY_TP='IND', NAME='SCOTT, HORACE EDWARD', CITY='HANCEVILLE', STATE='AL', ZIP_CODE='35077', EMPLOYER='HORACE F SEPTT', OCCUPATION='RETAIR', TRANSACTION_DT='06202024', TRANSACTION_AMT='100', OTHER_ID=None, TRAN_ID='SA17A.121694767', FILE_NUM='1810271', MEMO_CD=None, MEMO_TEXT=None, SUB_ID='4081620242013481558'),\n",
       " Row(CMTE_ID='C00828541', AMNDT_IND='A', RPT_TP='M7', TRANSACTION_PGI='P2024', IMAGE_NUM='202408159666204850', TRANSACTION_TP='15', ENTITY_TP='IND', NAME='BRUSA, S E', CITY='LODI', STATE='CA', ZIP_CODE='95242', EMPLOYER='HOMEMAKER', OCCUPATION='HOMEMAKER', TRANSACTION_DT='06202024', TRANSACTION_AMT='100', OTHER_ID=None, TRAN_ID='SA17A.121694255', FILE_NUM='1810271', MEMO_CD=None, MEMO_TEXT=None, SUB_ID='4081620242013468550')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEC.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd6bfde-9238-4b34-9a79-aa5affc42dd2",
   "metadata": {},
   "source": [
    "#### Step 16: Print Schema of the data\n",
    "* We do this to understand the schema of the data that was loaded. \n",
    "* By default it has been assumed to be string because we have called inferSchema=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efebedb2-ff1b-44ee-b395-4aa365aa19a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CMTE_ID: string (nullable = true)\n",
      " |-- AMNDT_IND: string (nullable = true)\n",
      " |-- RPT_TP: string (nullable = true)\n",
      " |-- TRANSACTION_PGI: string (nullable = true)\n",
      " |-- IMAGE_NUM: string (nullable = true)\n",
      " |-- TRANSACTION_TP: string (nullable = true)\n",
      " |-- ENTITY_TP: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- ZIP_CODE: string (nullable = true)\n",
      " |-- EMPLOYER: string (nullable = true)\n",
      " |-- OCCUPATION: string (nullable = true)\n",
      " |-- TRANSACTION_DT: string (nullable = true)\n",
      " |-- TRANSACTION_AMT: string (nullable = true)\n",
      " |-- OTHER_ID: string (nullable = true)\n",
      " |-- TRAN_ID: string (nullable = true)\n",
      " |-- FILE_NUM: string (nullable = true)\n",
      " |-- MEMO_CD: string (nullable = true)\n",
      " |-- MEMO_TEXT: string (nullable = true)\n",
      " |-- SUB_ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FEC.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb86fe-1d85-479d-b395-5a9937e239cd",
   "metadata": {},
   "source": [
    "#### Step 17: Importing functions and defining Schema for the data\n",
    "\n",
    "* `pyspark.sql.functions` Contains useful functions for manipulating DataFrame columns\n",
    "* `DateType`: Represents date values.\n",
    "* `FloatType`: Represents floating-point numbers.\n",
    "* `IntegerType`: Represents 32-bit signed integers.\n",
    "* `StringType`: Represents string values.\n",
    "\n",
    "**As mentioned in Step 13, we call inferSchema=False. Hence we define the datatype of Transaction_Amt below. \n",
    "* `cast(LongType())` Converts the column's data type to LongType (a long integer).\n",
    "* This step is required to get the most accurate output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8887a33f-b0aa-4684-9064-264740b0cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col, desc, format_number\n",
    "from pyspark.sql.types import DateType, FloatType, IntegerType, StringType, LongType\n",
    "\n",
    "FEC = sqlc.read.csv(\"JunJul_24.csv\", header=True, inferSchema=False)\n",
    "\n",
    "# Convert transaction amount\n",
    "FEC = FEC.withColumn(\"TRANSACTION_AMT\", \n",
    "            col(\"TRANSACTION_AMT\").cast(LongType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7218190-3751-47f9-8ff1-2f9500a989be",
   "metadata": {},
   "source": [
    "#### Step 18: Part 2 -> Question 1\n",
    "\n",
    "1. The contributor’s name, the committee ID the contributor contributed to, and the transaction amount for all contributions above $5000. Order the records by the transaction amount in descending order. Only show the top 10 rows.\n",
    "\n",
    "**Code Explanation**\n",
    "* `filter(\"TRANSACTION_AMT > 5000\")` Only rows satisfying the condition (TRANSACTION_AMT > 5000) are retained.\n",
    "* `select` Reduces the DataFrame to only the necessary columns for the analysis, improving efficiency and readability\n",
    "* `desc(\"TRANSACTION_AMT\")` Sorts the TRANSACTION_AMT column in descending order.\n",
    "* `withColumn` Modifies the existing TRANSACTION_AMT column by applying formatting.\n",
    "* `format_number(\"TRANSACTION_AMT\", 2)` Formats the numeric values in the TRANSACTION_AMT column to include commas and two decimal places.\n",
    "* `show(10)` Displays the first 10 rows of the processed DataFrame.\n",
    "* `False` Prevents truncation of column values, ensuring the full content is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41a41ac1-fd7b-4e4b-94e0-0b4593dcc218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+---------+---------------+\n",
      "|NAME                             |CMTE_ID  |TRANSACTION_AMT|\n",
      "+---------------------------------+---------+---------------+\n",
      "|MELLON, TIMOTHY                  |C00825851|50,000,000.00  |\n",
      "|ONE NATION                       |C00571703|18,400,000.00  |\n",
      "|SECURING AMERICAN GREATNESS      |C00881805|15,000,000.00  |\n",
      "|FUTURE FORWARD USA ACTION        |C00669259|15,000,000.00  |\n",
      "|CLEMENT, CHRISTINA, HH LOREN REV.|C00857128|12,000,000.00  |\n",
      "|BLOOMBERG, MICHAEL R.            |C00495028|10,000,000.00  |\n",
      "|SINGER, PAUL E.                  |C00504530|10,000,000.00  |\n",
      "|SINGER, PAUL ELLIOTT             |C00571703|10,000,000.00  |\n",
      "|BRICK BY BRICK FOUNDATION        |C00631549|5,000,000.00   |\n",
      "|AMERICAN ACTION NETWORK          |C00504530|5,000,000.00   |\n",
      "+---------------------------------+---------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "FEC.filter(\"TRANSACTION_AMT > 5000\") \\\n",
    "   .select(FEC['NAME'], FEC['CMTE_ID'], FEC['TRANSACTION_AMT']) \\\n",
    "   .orderBy(desc(\"TRANSACTION_AMT\")) \\\n",
    "   .withColumn(\"TRANSACTION_AMT\", format_number(\"TRANSACTION_AMT\", 2)) \\\n",
    "   .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7890f8c-67b1-48de-8f28-3e2eb9570516",
   "metadata": {},
   "source": [
    "#### Step 19: Part 2 -> Question 2\n",
    "\n",
    "2. Find the name of the individual and their total contributions. Order the records by the total transaction amount in descending order. Only show the top 10 rows.\n",
    "\n",
    "**Code Explanation**\n",
    "* `groupBy(\"NAME\")` Groups rows in the DataFrame by the NAME column (contributor's name).\n",
    "* `agg()` Specifies the aggregation operation to be performed on each group created by groupBy.\n",
    "* `sum_col(\"TRANSACTION_AMT\")` Sums the values in the TRANSACTION_AMT column for each contributor (NAME).\n",
    "* `.alias(\"TOTAL_CONTRIBUTION\")` Renames the aggregated column to TOTAL_CONTRIBUTION.\n",
    "* `orderBy(desc(\"TOTAL_CONTRIBUTION\"))` Sorts the DataFrame in descending order of the TOTAL_CONTRIBUTION column.  \n",
    "* `format_number(\"TOTAL_CONTRIBUTION\", 2)` Adds commas as thousand separators, for better view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b686c6f8-0721-4c93-bc8d-58d0091188ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------------+\n",
      "|NAME                             |TOTAL_CONTRIBUTION|\n",
      "+---------------------------------+------------------+\n",
      "|MELLON, TIMOTHY                  |55,003,300.00     |\n",
      "|ONE NATION                       |18,400,000.00     |\n",
      "|FUTURE FORWARD USA ACTION        |15,190,058.00     |\n",
      "|SECURING AMERICAN GREATNESS      |15,000,000.00     |\n",
      "|MUSK, ELON                       |14,950,000.00     |\n",
      "|CLEMENT, CHRISTINA, HH LOREN REV.|12,000,564.00     |\n",
      "|SINGER, PAUL E.                  |10,150,000.00     |\n",
      "|BLOOMBERG, MICHAEL R.            |10,014,900.00     |\n",
      "|SINGER, PAUL ELLIOTT             |10,000,000.00     |\n",
      "|AMERICAN ACTION NETWORK          |7,500,000.00      |\n",
      "+---------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as sum_col, desc, format_number\n",
    "\n",
    "FEC.groupBy(\"NAME\") \\\n",
    "   .agg(sum_col(\"TRANSACTION_AMT\").alias(\"TOTAL_CONTRIBUTION\")) \\\n",
    "   .orderBy(desc(\"TOTAL_CONTRIBUTION\")) \\\n",
    "   .withColumn(\"TOTAL_CONTRIBUTION\", format_number(\"TOTAL_CONTRIBUTION\", 2)) \\\n",
    "   .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72f5ca6-f41f-427a-9cf6-652fd08c4b9c",
   "metadata": {},
   "source": [
    "### Part 3: SQL with Data Frame (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec959ea-75e6-46f0-ac34-d72f5173e0d6",
   "metadata": {},
   "source": [
    "#### Step 20: Run SQL queries on the data using PySpark's SQL engine.\n",
    "\n",
    "* `createOrReplaceTempView` Creates a temporary SQL view named fec_data from the DataFrame FEC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb93685a-03e6-43f0-bbe9-79b16b1d49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEC.createOrReplaceTempView(\"fec_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b0d3c-d03f-44eb-bc04-9bd27ffb75fb",
   "metadata": {},
   "source": [
    "#### Step 21: Run line count\n",
    "\n",
    "* We run this to ensure the data load is successful and there are no rows missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1be817a4-c27a-4294-9958-b808d06d7ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3846794|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sqlc.sql(\"SELECT COUNT(*) FROM fec_data\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608f0e7-f1b8-4828-97a7-8fefa1464507",
   "metadata": {},
   "source": [
    "#### Step 22: Part 3 -> Question 1\n",
    "* Find the individual contributor’s name, the contributor's occupation, and the transaction amount. Order the records by the transaction amount in descending order. Only show the top 10 rows.\n",
    "\n",
    "**Code Explanation**\n",
    "* `sqlc.sql` is used to call sql to perform analysis \n",
    "* `SELECT` Specifies the columns to retrieve from the view:\n",
    "* `NAME` The contributor's name.\n",
    "* `OCCUPATION` The contributor's occupation.\n",
    "* `TRANSACTION_AMT` The transaction amount of the contribution.\n",
    "* `FROM fec_data` Indicates the data source (the fec_data temporary view).\n",
    "* `ORDER BY TRANSACTION_AMT DESC` Sorts the rows by the TRANSACTION_AMT column in descending order.\n",
    "* `LIMIT 10` Restricts the output to the top 10 rows after sorting.\n",
    "* `.show()` Displays the results of the query in the console.\n",
    "* `n=10` Displays up to 10 rows.\n",
    "* `truncate=False` Ensures full column values are displayed without truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1b54151-61dd-4599-adc6-800cea5df13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-------------+---------------+\n",
      "|NAME                             |OCCUPATION   |TRANSACTION_AMT|\n",
      "+---------------------------------+-------------+---------------+\n",
      "|MELLON, TIMOTHY                  |INVESTMENTS  |50000000       |\n",
      "|ONE NATION                       |NULL         |18400000       |\n",
      "|SECURING AMERICAN GREATNESS      |NULL         |15000000       |\n",
      "|FUTURE FORWARD USA ACTION        |NULL         |15000000       |\n",
      "|CLEMENT, CHRISTINA, HH LOREN REV.|SELF EMPLOYED|12000000       |\n",
      "|BLOOMBERG, MICHAEL R.            |FOUNDER      |10000000       |\n",
      "|SINGER, PAUL E.                  |PRESIDENT    |10000000       |\n",
      "|SINGER, PAUL ELLIOTT             |PRESIDENT    |10000000       |\n",
      "|BRICK BY BRICK FOUNDATION        |NULL         |5000000        |\n",
      "|AMERICAN ACTION NETWORK          |NULL         |5000000        |\n",
      "+---------------------------------+-------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sqlc.sql(\"\"\"\n",
    "    SELECT NAME, OCCUPATION, TRANSACTION_AMT\n",
    "    FROM fec_data\n",
    "    ORDER BY TRANSACTION_AMT DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c127712-7abe-4efd-8f4a-ee1da2ae6401",
   "metadata": {},
   "source": [
    "#### Step 23: Part 3 -> Question 2\n",
    "* The top 10 committees based on their total individual contributions on a given date. Provide the committee ID, the transaction date, and the transaction amount. Order the output by the total transaction amount in descending order. \n",
    "\n",
    "**Code Explanation**\n",
    "* `SELECT` Specifies the columns to retrieve and the aggregation\n",
    "* `AS total_transaction_amount` The result is aliased as total_transaction_amount.\n",
    "* `FROM fec_data` Indicates the data source (the fec_data temporary view).\n",
    "* `GROUP BY CMTE_ID, TRANSACTION_DT` Groups the rows by CMTE_ID and TRANSACTION_DT\n",
    "* `ORDER BY total_transaction_amount DESC` Sorts the rows by the total_transaction_amount column in descending order.\n",
    "* `LIMIT 10` Restricts the output to the top 10 rows after sorting.\n",
    "* `.show()` Displays the results of the query in the console.\n",
    "* `n=10` Displays up to 10 rows.\n",
    "* `truncate=False` Ensures full column values are displayed without truncation.\n",
    "\n",
    "**NOTE:** As we can see the output in TRANSACTION_DT is in the original format as our data, hence we call inferSchema=False in the earlier steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "884d85b3-4538-421c-a98f-1dffdfcd4ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------------------+\n",
      "|CMTE_ID  |TRANSACTION_DT|total_transaction_amount|\n",
      "+---------+--------------+------------------------+\n",
      "|C00825851|07152024      |50000000                |\n",
      "|C00401224|07212024      |26894750                |\n",
      "|C00401224|07222024      |26418477                |\n",
      "|C00571703|06282024      |18650000                |\n",
      "|C00401224|07232024      |16219539                |\n",
      "|C00669259|06262024      |16000000                |\n",
      "|C00703975|07212024      |15996549                |\n",
      "|C00881805|07182024      |15000000                |\n",
      "|C00744946|07222024      |14057883                |\n",
      "|C00825851|06262024      |14000000                |\n",
      "+---------+--------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sqlc.sql(\"\"\"\n",
    "SELECT CMTE_ID, TRANSACTION_DT, SUM(TRANSACTION_AMT) AS total_transaction_amount\n",
    "FROM fec_data\n",
    "GROUP BY CMTE_ID, TRANSACTION_DT\n",
    "ORDER BY total_transaction_amount DESC\n",
    "LIMIT 10\n",
    "\"\"\").show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e9f2d-2093-4a8f-ba77-75fe13040392",
   "metadata": {},
   "source": [
    "#### Conclusion: Key Insights from Political Contributions Analysis\n",
    "* The dataset reveals significant insights into political contributions, highlighting patterns in high-value donors, PACs - Political Action Committees, and geographic trends.\n",
    "\n",
    "**Top States for Contributions:**\n",
    "* **California (CA), Texas (TX), and Florida (FL)** dominate both count and total contributions, reflecting their political and economic influence.\n",
    "* **New York (NY) and Illinois (IL)**, known for financial hubs and influential donors, also rank highly.\n",
    "\n",
    "**High-Value Donors and PACs:**\n",
    "* Individuals like **Timothy Mellon** ($55 million) and **Michael Bloomberg** contribute heavily, underscoring the disproportionate influence of wealthy individuals.\n",
    "* Organizations like **One Nation** and **Securing American Greatness** play a pivotal role in aggregating large funds, emphasizing the power of PACs in elections.\n",
    "\n",
    "**Occupational and Sectoral Influence:**\n",
    "* High-value contributors predominantly come from sectors like **Investments and Entrepreneurship**, showcasing the financial industry's role in political funding.\n",
    "\n",
    "**Top Committees:**\n",
    "* Committees like **C00825851** received record single-day contributions (e.g., $50 million on 07/15/2024), often correlating with campaign milestones.\n",
    "\n",
    "**Patterns and Trends:**\n",
    "* **Dominance of Key States:** Larger, wealthier states contribute significantly to campaign funding.\n",
    "* **Role of PACs:** Super PACs like One Nation aggregate substantial amounts to influence elections.\n",
    "* **Influence of Wealthy Individuals** High-profile donors like Timothy Mellon shape campaign financing disproportionately.\n",
    "* **Geographic Concentration:** Contributions are unevenly distributed, favoring states with economic and political power.\n",
    "\n",
    "**Insights from FEC Validation:**\n",
    "* **Timothy Mellon**’s ($50) million aligns with his history of substantial conservative funding.\n",
    "\n",
    "* Insights validated from https://www.politico.com/news/2024/08/20/timothy-mellon-maga-inc-00175249. According to FEC records, Timothy Mellon made a significant contribution of $50 million to the pro-Trump super PAC, Make America Great Again Inc., in July 2024. This aligns closely with the query result, confirming his substantial financial involvement in political campaigns.\n",
    "\n",
    "* **One Nation**’s $18.4 million contribution reflects its role as a key political player.\n",
    "\n",
    "* Insights validated from https://www.fec.gov/data/committee/C00468447. One Nation is a registered political action committee (PAC) with the FEC. While specific contribution amounts may vary over time, One Nation has a history of substantial financial involvement in political campaigns, consistent with the query result.\n",
    "\n",
    "**Reflections:**\n",
    "* **Concentration of Wealth:** The dominance of wealthy individuals and PACs raises concerns about voter equity in campaign influence.\n",
    "* **Transparency Needs:** The scale of contributions underscores the importance of campaign finance laws and the need for reforms to ensure fairness.\n",
    "\n",
    "**Final Conclusions/Key Takeaways:**\n",
    "\n",
    "* The outcomes of the query align closely with expectations based on historical trends in campaign financing.\n",
    "* States like California, Texas, and Florida consistently dominate in contributions due to their large populations, economic strength, and political significance\n",
    "* High-value donors such as Timothy Mellon and PACs like One Nation reflect the expected influence of wealthy individuals and organizations.\n",
    "* Scale of contributions, including Mellon's $55 million and the $50 million single-day donation to C00825851, underscores the growing reliance on ultra-wealthy donors and PACs.\n",
    "* This analysis serves as a reminder of the importance of transparency and regulation in maintaining a fair democratic process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
